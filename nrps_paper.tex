\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[export]{adjustbox}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Demonstrating NeuRoShambo: A Live Event-Based RoShambo Classifier Using the TENNLab Neuromorphic Framework %RISP\\
}

\author{
\IEEEauthorblockN{Jackson Mowry}
\IEEEauthorblockA{\textit{EECS Department} \\
\textit{University of Tennessee}\\
Knoxville, TN USA\\
jmowry4@vols.utk.edu, Orcid 0009-0005-8290-3194}
\\\
\IEEEauthorblockN{George Evans}
\IEEEauthorblockA{\textit{EECS Department} \\
\textit{University of Tennessee}\\
Knoxville, TN USA\\
gevans16@vols.utk.edu, Orcid 0000-0001-5522-8123}
\and
\IEEEauthorblockN{Seoyoung An}
\IEEEauthorblockA{\textit{EECS Department} \\
\textit{University of Tennessee}\\
Knoxville, TN USA\\
san5@vols.utk.edu, Orcid 0009-0002-8256-6376}
\\
\IEEEauthorblockN{Ria Patel}
\IEEEauthorblockA{\textit{EECS Department} \\
\textit{University of Tennessee}\\
Knoxville, TN USA\\
rpatel77@vols.utk.edu, Orcid 0000-0002-0723-7911}
}

\maketitle

\begin{abstract}
NeuRoShambo is a one-shot image classifier for determining which of the three Roshambo symbols is currently displayed. The classifier is implemented as a convolutional spiking neural network, which runs entirely in real-time. This network is initially trained as a traditional convolutional neural network then ``sharpened'' into a spiking neural network through the use of Whetstone. As input to the network, we provide events produced by an event-based camera. Its design balances throughput and overall latency to ensure consistent inference performance, even under the most challenging conditions. The entire system targets low size, weight, and power hardware, meaning NeuRoShambo can be deployed on cheap single-board computers. In this paper, we demonstrate our data collection and training methodology, as well as our real-time implementation.
\end{abstract}

\begin{IEEEkeywords}
event-based, neuromorphic, convolutional networks, machine learning, spiking neural network, event-based classification
\end{IEEEkeywords}

\section{Introduction}

Gesture recognition represents an exciting new set of possibilities for human-computer interaction where users can communicate directly with computational interfaces using their bodies. However, traditional gesture recognition methods, which utilize conventional frame-based cameras, suffer from an overwhelming amount of data to parse, blur, and poor frame rates. To overcome this issue, gestures can be captured via event-based cameras (EBCs), a neuromorphic implementation of a camera that interprets changes in incoming light intensity per pixel as event streams~\cite{Gallego_Delbruck_Orchard_Bartolozzi_Taba_Censi_Leutenegger_Davison_Conradt_Daniilidis_et_al._2022}.

The event data stream generated by the EBC is well suited for neuromorphic tasks, specifically Spiking Neural Networks (SNNs) which take encoded events known as spikes as inputs. This structure more closely mimics the function of the brain with much lower required energy. However, creating such networks by hand can be time-consuming.

% There exist several methods to generate SNNs for any given problem, and which approach is best used for each problem depends on the parameters of inputs and the size of the networks necessary. One solution is to use the Evolutionary Optimization for Neuromorphic Systems (EONS), which uses biologically inspired evolutionary methods such as mutation and crossover to evolve better networks from generation to generation, evolving the parameters such as leak, refractory period, nodes, weights, and synapses\cite{EONS}.

Classification tasks are the most-widely tested applications to demonstrate performance at the frontier of computer science. One simple example is identifying the three hand gestures in rock-paper-scissors, wherein the three options for every turn are always either rock, paper, or scissors. Each player picks one and shows their pick by making a distinct shape with their hands. Rock is a closed fist, paper is a flat hand, and scissors are shown by making a horizontal V-shape with the middle and pointer fingers. The popularity of the game, as well as the distinct hand movements for each option, make it a strong candidate as a classification application.

Most work involving EBCs as the input source requires a form of pre-processing and by extension down-sampling. The data from the camera must be processed by some metric, whether it is temporal or by each pixel. The amount of data taken by an EBC is also too large for most networks to handle, so the data must be limited without a significant loss of detail from the input image. The chosen methods for limiting and segmenting data have consequences on the trained networks; the training time and size of the network can be greatly dependent on the amount of data provided.

In this project, we train a convolution spiking neural network on a custom NEUROSHAMBO dataset, following a similar methodolgy to the one used to capture the ROSHAMBO17 data set~\cite{RPS_Dataset}, to perform a live demonstration of rock, paper, and scissors classification using the DAVIS346 EBC.

\section{Related Works}
\label{sec:related_works}
% More related works

Rizzo et al.\ have already demonstrated that SNNs can be easily integrated with EBC outputs in the context of a classification algorithm\cite{Rizzo_Mccombs_Haynie_Schuman_Plank_2023}. A point of consideration brought up in the work done by Rizzo et al.\ is how the event stream generated by the camera will be processed. There are no frames in the same way a conventional camera works, so methods are needed both to limit the incoming events to those occurring in the region and time of interest and to segment the data based on some metric.

The motivation to use a fully event based and neuromorphic implementation for gesture recognition is from the potential energy efficiency from the approach. Amir et al.\ found that their EBC combined with the TrueNorth neuro-synaptic processor achieved high accuracy levels with lower power draw applied to the DVSGesture. With a Convolutional Neural Network of over 1 million neurons, the network drew less than 200~mW of power~\cite{Amir_Taba_et_al._2017}. They used a DVS128 camera, which is an older model and thus has lower resolution than more modern versions. The amount of pixels to consider impacts the size and power draw of the network, depending on what pre-processing and downsampling methods are used. Although this particular approach differs from ours, it could serve as a good power benchmark for our own results.

Like other implementations of neural networks, some research has shown that multiple layers of SNNs can improve the classification accuracy of an SNN. For example, Fan et al.\ demonstrate that a multi-layer implementation of an SNN outperforms single-layer SNNs on the NCAR dataset\cite{Fan_Zhang_Liu_Li_Lu_2024}.

Another approach is a method called Whetstone, developed by Sandia National Lab that trains the binary, threshold-activation spiking neural networks using deep learning methods~\cite{Severa_2019}. By training using the dynamic activation function, Whetstone can design the SNN from the conventional deep-learning neural networks. Because Whetstone is compatible with Keras, the Keras model can be translated to a leaky integrate-and-fire neuron model for neuromorphic devices.

Other research has also shown that CNNs can be effective solutions for classification applications, specifically for rock-paper-scissors. Ahmed et al.\ make use of a technique called transfer learning combined with CNNs to develop networks capable of achieving high accuracy on a rock-paper-scissors classification dataset \cite{Ahmed_Khan_Iqbal_Ahmad_Abazeed_Alrababah_Khan_2023}. However, their CNN implementation still requires the use of down-sampling in order to make the incoming data small enough to be processed by the network.

% I can go into more detail here if needed, or move where it is
Some classification algorithms present difficulties to SNNs, which either become too difficult to effectively train or to achieve high accuracy. Corodne et al.\ propose a novel voxel cube method to encode the data that preserves the binary and temporal data with a low number of time steps. The voxel cube method presents an implementation of SNNs that effectively interprets real-world event data by creatively manipulating the dimensions for the data~\cite{9892618}.

Because the EBC data is sparser than the standard video frames, downsampling the camera data is a great option to train more efficiently. One of the methods is to use multiple neuromorphic networks that downsample the data by converting each frame to events and combining the downsampled data with the agent. Rizzo et al.\ used a chipping methodology where the input space is broken into sub-regions (chips) created for every \textit{s} pixel where s is the number of strides to filter the events. Then, a threshold can be selected to allow selecting regions which have an equal or greater count of constituient pixels on~\cite{downsampling}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/3_viz.png}
\caption{Visualization of 16$\times$ downsampled dataset. Top-left: rock; top-right: paper; bottom-center: scissors.}
\label{fig:viz_data}
\end{figure}

\section{Methods}

\subsection{ROSHAMBO Dataset}

The ROSHAMBO17 dataset provides both fixed-frame-rate video files and raw Address Event Representation (AER) data. The dataset was recorded using the DAVIS240C event-based camera. For preprocessing, we batched the raw AER events into frames by accumulating events over 40ms (equivalent to 25 frames per second). A pixel was considered ``on'' for a given frame if it registered an event of either polarity during the accumulation window. These event frames were subsequently downsampled to one-sixteenth of their original resolution to accelerate training and reduce model size. A visualization of the downsampled gestures is shown in Figure~\ref{fig:viz_data}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{figures/downsample.png}
\caption{Example of 16$\times$ downsampling with corresponding input/output network structure.}
\label{fig:downsample}
\end{figure}

To perform the downsampling, we apply an activation function to aggregate each group of 16 pixels into a single pixel. The input to each activation function was the number of ``on'' pixels within the group, and the output was a value in the range [0, 1]. We evaluated three activation functions:

\begin{itemize}
    \item \textbf{Linear}: Divides the count of active pixels by 16, resulting in a continuous value across [0, 1].
    \item \textbf{Hyperbolic tangent (tanh)}: Emphasizes regions with more than 50\% activation by mapping to higher-magnitude outputs.
    \item \textbf{Binary}: Outputs 1 if more than $N$ pixels are active, otherwise 0.
\end{itemize}

An illustrative example of this process and the resulting network structure is provided in Figure~\ref{fig:downsample}.

In our experiments, we found that the linear and tanh activation functions did not improve performance relative to the binary approach. Although the linear activation offers richer input encoding, it introduces complexity that is not easily overcome when dealing with the spiking model required to utilize a SNN. Ultimately, we selected the binary activation function, as it best reflects the discrete, event-driven behavior of spiking neural networks (SNNs).

\subsection{Custom NeuRoShambo Dataset}

\begin{figure}[!ht]
    \centering    \includegraphics[width=0.9\linewidth]{figures/our_viz.png}
    \caption{Dataset Visualization of 36x Downsampling. Top left rock, top right paper, bottom center scissors.}
    \label{fig:our_data}
\end{figure}

After testing the trained network, we found that the performance was not sufficient when utilizing a DAVIS346 camera for real-time inference. The DAVIS346 was downsampled at a rate of 36x to match the resolutions of the two cameras, which produced a final image of the same dimensions. Therefore, we decided to add our own training data to augment the training data present in ROSHAMBO17. We utilized the DAVIS346 camera to record three people (one female and two males) performing each of the three gestures under nine different conditions.

Through the augmentation of ROSHAMBO17 we achieved slightly higher accuracy both in training and real-time inference. However, we ultimately decided against combining both datasets as it increased training complexity. Using our own dataset allows showcasing the application as an initial proof-of-concept, which we can later generalize to a broader set of environmental conditions.

The DAVIS346 camera offers a higher spatial resolution (346 $\times$ 260 pixels) compared to the DAVIS240C (240 $\times$ 180 pixels). While the DAVIS346 provides higher spatial fidelity, it also revealed a key advantage of the DAVIS240C:\@ the ability to capture a scene with nearly the same resolving power while requiring less overall bandwidth to transmit fewer events. In our case, finer spatial details were unnecessary since the gestures occupied large regions of the frame, and the downsampled resolution of both cameras ended up being the same. Ultimately, the larger resolution of the DAVIS346 will prove to be useful in other application that require much higher levels of detail for the given task.

Each sample consists of 1,200 frames, or one minute of footage, which results in about 1GB of data per subject. The recordings were collected under varying environmental conditions, including changing light intensities and different backgrounds. For example, one subject performed the rock gesture in front of their body to reduce background noise, while another recorded in front of a TV (both turned on and off) to introduce different lighting conditions. There were total of 3 subjects (2 males and 1 female). To further diversify the dataset, we augmented the data by flipping all the collected frames, providing additional context for the model. During training, our model’s preprocessing pipeline also randomly rotates some of the images to introduce further variability and enable the model to learn a more robust set of features.

\subsection{Whetstone Implementation}

Using the TensorFlow Keras framework~\cite{keras} in conjunction with Sandia National Laboratories' Whetstone software~\cite{whetstone}, we explored initial convolutional spiking neural network (CSNN) architectures by training on the EBC frames described previously. To ensure compatibility with the TENNLab Whetstone translation tool, we restricted our models to a subset of supported layer types: 2D Convolution (with both ``valid'' and ``same'' padding), 2D Max Pooling, Dense, Flatten, and Softmax Decode layers.

We began by adapting a convolutional model originally used by the Whetstone team on the MNIST dataset. Our experimentation focused on varying architectural and training parameters, including the number and size of convolutional layers (e.g., kernel size, number of filters, padding type), dense layer sizes, batch normalization, dropout rates, and spiking activations. Training included early stopping and adaptive sharpening over 20 epochs, evaluating models based on accuracy, stability, overfitting, and size. From this process, we identified a group of promising base models for extended training and fine-tuning of hyperparameters such as sharpening rate, learning rate, and optimizer choice.

The final training was conducted over 1,600 epochs using the Adam optimizer~\cite{adam}. As required by Whetstone, our models incorporated a sharpening callback that gradually transforms a bounded ReLU (BReLU) activation into a binary function, with a threshold at $x = 0.5$. We initiated sharpening after 1,000 epochs to allow sufficient pre-sharpening learning.

Our final model architecture includes two 2D convolutional layers with 16 and 32 filters, respectively, of size $3 \times 3$, followed by a $2 \times 2$ max pooling layer each. The output is flattened and passed through 2 fully connected layers with 64 and 3 units, respectively. Each trainable layer is followed by a \texttt{SpikingBRelu} activation, which enables progressive sharpening during training, as illustrated in Figure~\ref{fig:nn_structure}. Finally, the decoding layer is implemented as a softmax, giving us a single classification per inference pass.

\begin{figure}[!ht]
    % \centering
    \includegraphics[width=0.8\linewidth, right]{figures/network_structure.png}
    \caption{Keras Model Spiking Neural Network Structure}
    \label{fig:nn_structure}
\end{figure}

\subsection{RISP Implementation}
The TENNLab software framework~\cite{framework} provides an application that can translate Whetstone networks trained within Keras to standalone SNNs~\cite{tennlab_whetstone}. Then, these SNNs can be run on any of the platforms supported by the TENNLab software framework. For our application we decided to target the Reduced Instruction Spiking Processor (RISP) neuroprocessor~\cite{RISP}. RISP notably supports leaky integrate-and-fire neurons, which allow the Whetstone translation tools to produce much more efficient networks, without the need to manually apply leak through separate portions of the network. Additionally, RISP supports floating-point values for neuron thresholds, charges, and synaptic weights, allowing a direct translation without loss of precision. RISP supports all three major platforms for network deployment, including microcontrollers, single-board computers (SBCs), and field programmable gate arrays (FPGAs)~\cite{risp_on_hardware}.

To implement the down-sampling approach into neuromorphic hardware, described above, we need to ``graft'' a smaller downsampling network onto its input neurons. This downsampling network requires no training and efficiently maps each group of 36 neurons to the corresponding input neuron for the trained network. The thresholds of these input neuron are set to $3$ to ensure that at least $3$ spikes are required for the pixel to be considered in the downsampled image.

We provide the trained model, described above, to the TENNLab Whetstone application resulting in a standalone SNN. With the layer structure described above the SNN has 134,711 neurons and 6,951,366 synapses. The average synapse has $\sim$52 outgoing connections, with the most densely connected neurons having 288 connections. Our network consists of $6$ neuromorphic layers, meaning that we require 7 simulation timesteps from the time we apply input to the time we have a classification.

The generated network is designed to support a feature known as pipelining. In neuromorphic computing we add a temporal component not present in traditional neural networks. This takes the form of $delay$, in which each synaptic connection has some form of latency in propagating a spike to its post-synaptic neuron. Utilizing a traditional layer structure, commonly found in deep neural networks, means that each layer is computed one at a time, with the resulting activations fed to the next layer on the following timestep. This would normally lead to reduced throughput, as the network would need to run for $7$ timestep to produce a single classification.

However, due to the SNNs structure and RISP supporting LIF neurons, we can apply an entire frame of input at each timestep, with the corresponding classification occurring $7$ timesteps later. Utilizing this strategy we have $7$ frames ``in-flight'' at once, greatly increasing throughput without increasing latency.

\subsection{Hardware Implementation}
In this work, we deployed our trained network on a Raspberry Pi 4 Model B single-board computer (SBC) and tested it using a DAVIS346 event-based camera (EBC). Although the SBC is capable of running the full TENNLab software simulator, it contains a number of superfulous features. Therefore, an embedded version of the software framework is under development to enable more efficient deployment on microcontroller or SBC hardware~\cite{gullett}.

The embedded RISP framework supports two implementations: ``sparse-RISP'' and ``dense-RISP''. Sparse-RISP efficiently simulates sparsely connected networks with low activity by computing only on firing neurons, adhering to a true event-driven model. In contrast, dense-RISP is designed for networks with higher connectivity and activity levels. It maintains the full network state using a ``charge table'' and updates the state of every neuron at each timestep. While it can be computationally intensive compare to spare-RISP, especially at lower activity, this approach enables higher-speed simulation for our application. For this work we selected dense-RISP as it provided performance 6.87x-8.88x higher than sparse-RISP.

To interface with the event-based camera, we employed Inivation’s \texttt{DV-Processing} library~\cite{dv-processing}, which allows batching of events into full-resolution frames at a user-specified rate. We chose to output frames at the same 20 FPS as was used in training. These frames are then fed into the network as input.

Once our highest performing model was converted into a standalone SNN it was deployed on our SBC to enable real-time inference. The setup consisted of the Raspberry Pi connected to a DAVIS346 via USB. The Pi was connected to a TV to allow participants to both visualize what the camera was seeing, and to view the current predicted hand gesture, which can be seen in Figure~\ref{fig:hardware_implementation}.

\section{Results}
\label{sec:results}

\begin{figure}[!ht]
    % \centering
    \includegraphics[width=0.97\linewidth, right]{figures/accuracy_plot.png}
    \caption{Training and Testing Accuracy}
    \label{fig:accuracy_plot}
\end{figure}

\begin{figure}[!ht]
    % \centering
    \includegraphics[width=0.97\linewidth, right]{figures/loss_plot.png}
    \caption{Training and Testing Loss}
    \label{fig:loss_plot}
\end{figure}

Figure~\ref{fig:accuracy_plot} illustrates the training and validation accuracy over 1,000 epochs. Both metrics peak around epoch 100, with training accuracy approaching 100\% and validation accuracy reaching approximately 90\%. Overall, accuracy remains stable throughout the majority of training. The figure also provides insight into potential overfitting: the gap between training and validation accuracy is small (around 5\% to 7\%), indicating minimal overfitting.

After epoch 1,000, both training and validation accuracy begin to decline. This behavior is caused by the sharpening callback used during Whetstone training, which progressively converts the BReLU activation function into a binary step function. This transformation introduces instability into the network as some level of precision is lost. However, the adaptive sharpening process consists of many tunable parameters, which enable the network to relearn, even after sharpening has been applied to the initial layers. This can be seen around epochs 1200 and 1300, where the network experiences a sharp drop in accuracy, followed by a gradual increase to some fraction of the pre-sharpening accuracy. During these periods sharpening is paused to allow relearning.

Figure~\ref{fig:loss_plot} shows the loss values for both training and validation sets. As expected, both losses decrease during the initial epochs. The training loss converges to approximately 0.05, while the validation loss levels off around 0.22, indicating higher prediction error on the validation set compared to the training set. Sharpening in Whetstone progresses through the layers in a forward direction, meaning the last layer to be sharpened is the output layer. This is notable as once the network is fully sharpened further training with back-propagation is no longer viable. We observe this around epoch 1600 where loss quickly increases without the ability to recover as seen when sharpening other layers.

\subsection{Power Consumption}
Power was measured using a P3 KILL A WATT P4400~\cite{killawatt}, with the Pi's power supply being connected directly to the measurement device. The DAVIS346 EBC was powered entirely through the Pi's USB interface, and draws approximately 1~W on its own. We measured total system power draw at idle and under full application load. The idle power draw was 4.5~W, increasing to 6.5~W under load.

\subsection{Performance}
\begin{figure}[!ht]
    % \centering
    \includegraphics[width=0.9\linewidth, center]{figures/hardware_implementation_final.png}
    \caption{Hardware Implementation Demo}
    \label{fig:hardware_implementation}
\end{figure}
The network consistently identified the ``rock'', ``paper'', and ``scissors'' gestures with high accuracy. Among the three, ``scissors'' was classified most reliably, followed by ``rock'', while ``paper'' exhibited the lowest classification accuracy. As shown in Figure~\ref{fig:hardware_implementation}, the system displays the predicted gesture on-screen alongside the corresponding camera frame visualization. To reduce the influence of noise we use a sliding window of the last 10 predictions to give a more stable classification.

When no spikes occur on the output layer the system outputs ``scissors'' as a default classification. This is due to the softmax decode layer's tie-breaker mechanism which breaks ties by choosing the first output neuron of the group. The same behavior occurs if two or more of the output neurons were to spike.

With the buffering mentioned above our system incurs $10$ frames of latency in the worst case. These $10$ frames are in addition to the $7$ required to complete one forward pass through the network, resulting in less than a second of latency at 20 FPS.

\section{Discussion and Conclusion}
\label{sec:conclusion}
In this work, we successfully demonstrated real-time classification using a convolutional spiking neural network (CSNN) on a custom NeuRoShambo dataset. Leveraging the TENNLab framework's RISP neuroprocessor, we deployed our network on a Raspberry Pi 4 Model B, and interfaced with a live event-based camera for input. The system achieved real-time inference with low power consumption, operating at 6.5 W during active processing.

\section{Future Work}
\label{sec:futurework}
Future efforts will focus on optimizing the model architecture to reduce its size, enabling faster inference and improved throughput by allowing us to increase the frame rate. The current network's size contributes to increased processing time and latency, which we aim to mitigate.

An additional avenue for exploration is creating a smaller network architecture would also allow the possibility of implementing the network on neuromorphic hardware or microcontrollers with lower storage. For this to be feasible with current state-of-the-art hardware we would also likely need to perform quantization on the SNN to allow to integer only computation.

Additionally, we plan to expand our dataset to include more subjects and a broader range of environmental conditions. A key limitation of our current dataset is its limited duration—only 45 minutes of recordings—which restricts the model's generalizability. Increasing the diversity and volume of training data will be critical for improving robustness.

Finally, we aim to apply the same workflow methodology to enhance the CSNN model developed by Patel et al.~\cite{patel2024}, which performs classification of American Sign Language using an event-based dataset. This would further validate the applicability and scalability of our approach across different spatiotemporal classification tasks.

\bibliographystyle{ieeetr}
\bibliography{refs}


\vspace{12pt}


\end{document}

%% Local Variables:
%% languagetool-local-disabled-rules: ("EN_UNPAIRED_QUOTES" "CURRENCY")
%% End:
