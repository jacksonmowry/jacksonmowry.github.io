#+title: Operating Systems

* Chapter 2
- We have abstractions that allow a single program to behave as the only process on the machine
- Mechanisms are how is the code implemented
- Policies are how we decide when to use the mechanisms
- Primary way we expose resources is through virtualization
  - CPU, RAM, disk
  - Make virtual forms for the program to use
** Evolution
- Early OS was just a lib, only one process at a time
- Then we gain protections, raw data storage is dangerous
- Then we gained multiprogramming. Running more than one job at a time
- As we talk about various OS pieces, we will show the evolution into the modern versions .

* Chapter 8
** MLFQ: Multilevel Feedback Queue
- Multiple queues
- each is assigned a priority level
- When we make a decision we go to highest priority
  - For jobs within the same queue we use round-robin

*** Rules
1. If priority(A) > priority(B) => A
2. If priority(A) == priority(B) => Round-robin
3. When a job enters it is at highest priority
4. If it uses its entire timeslice its priority is reduced, no matter how many times it gives up the CPU
5. If it gives up the CPU before its timeslice is up it stays
6. After some time point S, move all jobs in the system to the topmost queue

** How to select Priority?
- Job that often yields (getting keyboard input) should be prioritized for quicker response time
- If a job keeps making syscalls, we should priortize it as it is probably an interactive process
- If it uses the cpu a lot we should prioritize turn-around time

But job priority cannot be entirely static, it must be able to change over time.

** How to change Priority?
- Give each job an allotment, this is the amount of time it can exist in a queue without reducing its priority
- When a job begins it is placed at the highest priority
- If it uses its entire timeslice its priority is reduced
- If a job gives up the CPU before running through its allotment it can stay at the same priority

Highly interactive jobs will always stay at the highest priority because they never use up an entire timeslice without making a syscall

A program can also game the scheduler by always issuing IO right before the time slice is up

A program can also change its behavior from CPU intensive to IO intensive

** Priority Boost
Process could be getting starved, but if we boost everything, they will all then get a fair chance to run again

** Better Accounting
- To prevent gaming, change rule 4
- If you use up your 10ms timeslice (even across schedules) then you move down

* Chapter 9
Proportional Share Scheduling

** A New Metric
- Tickets represent the share of a resource that an entity should recieve
- The percent of tickets it hold represent the % they get

*** Lottery
- Draw a random ticket, schedule the process that holds that ticket
- More tickets you have the more likely you are to be scheduled

*** Ticket Currency
- Allows a user to create their own currency
  - Global currency and then whatever currency each user creates

- Example both A and B are given 100 global tickets
  - A runs A1 and A2
    - A1/A2 both get 500 A bucks
  - B runs B1
    - B1 gets 10 B bucks

  - A1 and A2 are getting 50 global tickets each
  - B1 gets 100 global tickets

*** Ticket Transfer
- allows a process to lend tickets to another process
- Client/server running on same machine
  - Client sends request, wait on server
  - So it also send some of its tickets over to the server as well
  - The server will send them back when done

*** Ticket Inflation
- Can just grow its own ticket amount, only really makes sense in a co-op environment

*** Impl
- Good RNG
- List to hold processes
- Total # of tickets

- Generate number N
- Traverse list, add up ticket values
- Winner once the total is greater than N

- Linear time

*** Fairness
- 2 jobs of the same length how fair is the scheduler?
- Randomness affects short jobs
- Fairness = job_finish_1 / job_finish_2
- Want fairness to be 1

** Stride Scheduling
- Randomness isn't fair
- Deterministic fair scheduler
- Stride = (some large number) / tickets
  - Inverse in proportion to the number of tickets it has
- Each process has a running pass value starting at 0
- When a process is scheduled, increment its pass by stride
- Always schedule the process with the lowest pass breaking ties arbitrarily

- Why do we even do this?
  - We now have global state, lottery doesnt
  - What happens if we add a new process? What should its pass be? Do we start at 0, or start at average

** Linux Completely Fair Scheduler
- Highly efficient and scalable fair-share scheduler
- Aims to spend very little time making choices
- Important to not waste resources
  - Google used 5% of CPU time scheduling
- Reducing overhead is a key goal in modern schedulers
- Goal is to divide the CPU evenly among all competing processes
- It does so with a virtual runtime (vruntime)

*** CFS: Basic Operation
- Each process runs an accumulates vruntime
- Always picks the process with lowest vruntime
- CFS varies with sched_latency
  - Represents the largest time slice size possible
  - Determined by sched_latency/number of processes

*** Niceness
- Adds weighting to the time slice calculation
- time slice = Poriton of all processes running * max time slice
- vruntime = previous vruntime + time just ran * weighting based on niceness

$\frac{1024}{2048} * \mathtt{sched\_latency}$

$\mathtt{previous\_vr} + \frac{\mathtt{0 weight}}{\mathtt{your weight}} * \mathtt{sched\_latency}$

$100 + \frac{1024}{820} * 10 = 110$

*** Using Red-Black Tree
- List is O(n), tree is not
- Why not a heap

*** IO & Sleep
- When a job wakes up its vruntim is set to the maximum of its own vruntime and the minimum in the tree
  - It will be set to the min in the tree essentially
  - Still gets scheduled first, but wont get a ton more time
- This avoids starvation at the cost of not being fair to frequently sleeping processes

*** Other Fun
- Cache performance
- Multiple CPUs
- Large groups of processes

* Virtualizing Memory Chapter 13
** Early Systems
- OS was just a library of functions
- A single program used the entire memory available

** Multiprogramming and Time sharing
- Multiple processes, maximize cpu usage
- Time share
  - we need programs to be interractive

*** Time Sharing
- One way is context switching, including all physical memory to/from disk
- Brutally slow, especially as memory grows, we want to leave processes in memory and only save/load registers (much faster)
- We need protection to segment memory

** Address Space
- We need an abstraction on top of the physical memory
- The address space of a process contains all the memory state
  - Code, stack, heap, and others
- We need to make each process appear to have its own address space
*** Layout
- Code
- Heap
  - grows down
- Stack
  - grows up

** Goals of Virtual Memory
- Transparancy
- Efficiency
- Protection

* Chapter 15 Address Translation
** How to Efficienctly and Flexibly Virtualize Memory
- Effiecient
- Flexible
- Access protection
- Restrictions

** Assumptions
- Address space in placed contiguously in physical memory
- Address space fits in memroy
- Address spaces are all of equal size

** Translation
All of the following addresses are within our processes address space, not physical memory
1. Fetch instructions at address 128
2. Exectue this instruction (load address 15kb)
3. Fetch instruction at address 132
4. Execute this instruction (no load/store)
5. Fetch the instruction at address 135
6. Execute the instruction (store address 15kb)

** Dynamic Relocation
- Base and bounds (dynamic relocation)
  - 2 hardware registers base and bounds (beginning of mmu)
- Programs assume they're loaded at address 0x0
- When the OS places the program in rabe is sets base/bounds
  - Physical address = virtual address + bas
- Bounds registers prevents us from going past the end of our addres space

** Hardware Support So Far
| Hardware                          | Notes                                              |
| Privlige                          | Prevent user-mode from executing priv instructions |
| Base/Bounds                       | Address translation                                |
| Translate virtual address         | quick and make sure is valid                       |
| Instruction to update base/bounds | OS only functions                                  |
| Handle exceptions                 | if user accesses oob                               |
| Raise exceptions                  | throw when oob                                     |


* Chapter 26 Concurrency Introduction
- Multiple program counters
  - and it's own set of register
  - yet they share the same addr space
  - Own stack, which complicates the addr space
- Why?
  - Parallelism
  - Avoid blocking due to IO
- Possible Outcomes of Thread Creation
  - Non-deterministic creation and running of threads, relies on the OS scheduler
- Shared Data
- Heart of the Problem -- Uncontrolled scheduling
  - When a thread gets interrupted during a critial section of code, non-atomic operations essentially
  - Instead of making specialized atomic instructions we make a few primitive atomic instructions to create sync primitives
  - Waiting, sleeping/waking for cond vars
  - OS was the first concurrent program
    - 2 processes writing to the same file, how can we solve that?
* Chapter 28 Locks
- Basic idea is to create lock that can be "locked" or "unlocked" atomically

** Building a Lock
- Evaluating the performance/implementation of a lock
  - Mutual exclusion
  - Fairness (starvation of thread)
  - Performance (overhead)
    - single threaded overhead
    - multiple thread overhead on a single CPU
    - multpile thread overhead on multiple CPUs
- OG would turn off interrupts, meaning we physically cannot be interrupted
  - Pros
    - Simple
  - Cons
    - We have to trust all programs running
    - Doesn't scale to multiple CPUs
    - Interrupts off for an extended time can lead to lost interrupts
- Just using load/stores
  - Simple flag var
  - We either lock it or unlock it, but this is not being done atomically
  - Correctness
    - Setting the flag itself is not atomic, cannot be correct
    - Spin locks are horrible on single CPU systems
- Test and Set
  - This way works, but it's not great
  - atomic exchange
    - Returns the variables old value, and sets it to the new value atomically
    - If we get back 0, that means we just lock the lock (it is now 1)
    - If we get back 1, it was already locked, we must wait
    - Correctness -- yes
    - Fairness -- no
      - Nothing to guarantee that a thread eventually gets the lock
    - Performance
      - single CPU
        - Awful
        - Preempted within a critial section, the scheduler would go through every other thread before coming back to the only thread that can actually do work
      - Multiple CPU
        - Thread A grabs lock, B spins
        - Hopefully the critial section in short, thus it won't spin for long
- Compare and Swap
  - Differs from Test and Set b/c, if the value in the flag is equal to our expected value then we update it, and then we return the value of the flag always
  - Still produces a spin lock, waiting to see once the flag is zero
- Load-Linked and Store-Conditional
  - Only store to an address if no other intervening store has taken place
  - Has there been any update to the lock since I last did a load-linked?
    - If not we have success, otherwise failure
  - Load-Linked until it's zero
    - Then immedietly try to write to it
    - If somehow it got interrupted there will have been a write, therefore store conditional will fail and we need to start over
- Fetch and Add
  - Atomically increment a value while returning the old value
  - Use this to make a ticket lock (ticket value & turn value)
    - To acquire the lock fetch and add on the ticket value, this is your number
    - When the turn value equals your ticket, you have the lock
    - To unlock increment the turn value
  - This ensures fairness
** Avoiding Spinning
- We need OS support to prevent spinning
- Yield an entire time slice
  - But, with a ton of threads, we will possibly have to go through N-1 threads of context switches
  - also does not address fairness
- We use a queue to sleep instead of spin
  - This helps us avoid chance
  - Queue of threads that are waiting on the lock
  - Park/Unpark sleep and wake up, sleeping threads cannot be scheduled
- Putting something on the queue is really the only critial section, so we still spin, but this time should now be constant
- The thread who unlocks essentially wakes the first thread up, allowing it to be scheduled
- Setpark, indicates that I am about to park myself, so when unpark is called on this TID, the next park will be nop so you don't get put to sleep

** Futex
- By default this uses a futex under the hood
- Have a specific location in physcial memory, as well as a per-futex in-kernel queue
- Callers can use
  - futex_wait => sleep if value at addr is equal to expected
  - futex_wake => wakes one thread from the queue

- We attempt to spin shortly, but if we spin for too long we go to sleep and go into the kernel

* Chapter 30 Condition Variables
- Locks can't do it all
- How do we wait on something to happen?
  - Can spin
    - Horrible on a single CPU
    - OK on a multi cpu system, if the job is short
  - We really should sleep until some condition is met
- Building a condition variable
  - explicit queue that threads can put themselves on when some condition is not as they want it to be
  - When another thread changes that condition it can wake one of the threads in the queue
** Definition
- Must hold lock before calling wait
  - wait releases the lock and sleeps the calling thread atomically
  - When the thread is woken up, it will reacquire the lock
- Two cases
  - Either parent of child runs first
  - Do we still need the done variable?
    - Removing done can cause a race condition, the child may run first and signal to no one
** Producer/Consumer
- One or more of each of these types of threads
- Bounded buffer
  - put() assumes that the queue is empty, adds the value and sets count to 1
  - Get sets count to 0 and returns the value
  - Only want to put at 0, only get at 1
  - Producer grabs lock, waits if it is one, wakes up when buffer is empty
  - Consumer grabs lock, wait if it is 0, waks up when buffer has a value
*** Mesa Semantics
- Cond vars are just a hint that things may have changed, not a 100% that it changed
- We must always recheck when we recieve the signal
- if count == 1, while count == 1
* Semaphore
- 2 routines
  - sem_wait()
    - decrements
  - sem_post()
    - increments
    - Also wakes threads up
** Binary Semaphore
- Init to a value 0
- wait/post bounce back and forth between 1 & 0
** Solving Producer Consumer Problem
- Init semaphore empty to MAX
- Init semaphore full to 0
- Producer waits on empty, posts on full
- Consumer waits on full, posts on empty
- Wont work when MAX > 1
- Race condition between consumers and between producers
** Reader Writer Locks
- Several concurrent list operations, including inserts and simple lookups
- Inserts change state
- Lookup simply look at list
- Many people can look, as long as no one write
- RW Lock supports this
- First reader also grabs the write lock
- If they go to release and they're last, they can let go of the write lock
-

* Chapter 32 - Common Concurrency Problems
-

* Midterm 1 Review
** Stack VA->PA Translation
- 2 segments, 1024 total, 512 each
- Take the max possible size - offset = reverse offset
- As long as this is <= the bounds we're good
- Final address translation is base - reverse offset = PA
