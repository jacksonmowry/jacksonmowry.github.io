%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Code for SVM and Clustering Exam
% Generated by Gemini
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the 'article' class, 12pt font size
\documentclass[12pt]{article}

% --- PACKAGES ---
\usepackage[margin=1in]{geometry} % Set 1-inch margins
\usepackage{amsmath, amssymb}    % For math symbols, equations, and norms
\usepackage{enumerate}          % To customize list labels (e.g., a), b))
\usepackage{graphicx}           % To include images (though none are here)

\usepackage[many]{tcolorbox}

% --- CUSTOM COMMANDS ---
% Define a consistent command for vectors (bold font)
\newcommand{\vect}[1]{\mathbf{#1}}

% --- DOCUMENT START ---
\title{SVM and Clustering Exam (100 Points)}
% \author{Your Name Here} % Uncomment to add your name
\date{} % Suppress the date from appearing

\begin{document}

\maketitle

% --- INSTRUCTIONS ---

 \vspace{0.5em}
 \centerline{\textsc{\large UTK COSC 522: Machine Learning (Fall 2025)}}
 \vspace{0.5em}
 \centerline{OUT: Monday, Nov 17th, 2025} 
\centerline{\textbf{Due Date: Dec 5th, 2025, 11:59:59PM}}
\vspace{0.5em}
\noindent\textbf{Instructions:} Please answer the following questions clearly and show your work for any computations.\\

\hrule
\vspace{1em} % Add a little vertical space

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PART 1: SUPPORT VECTOR MACHINES (SVM)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Part 1: Support Vector Machines (SVM)}

\begin{enumerate}

% --- QUESTION 1 ---
\item \textbf{(15 points) Hyperplane Classification and Distance}

Given a 2D hyperplane defined by the weights $\vect{w} = (1, -2)^T$ and bias $b = 0$. The decision function is $f(\vect{x}) = \vect{w}^T\vect{x} + b$. Points with $f(\vect{x}) \ge 0$ are classified as $y = +1$, and points with $f(\vect{x}) < 0$ are classified as $y = -1$.

\begin{enumerate}[a)]
    \item (5 pts) For the point $\vect{x}_1 = (1, 1)$ with true label $y_1 = +1$, is this point correctly classified by the hyperplane? Show your calculation.
    \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
            %solution 
    \end{tcolorbox}
        
    \item (5 pts) For the point $\vect{x}_2 = (1, 0)$ with true label $y_2 = +1$, is this point correctly classified by the hyperplane? Show your calculation.
    
    \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
        
    \item (5 pts) Calculate the signed geometric distance of the point $\vect{x}_3 = (-1, 2)$ from the hyperplane (see the lecture slides). 

    \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
        
\end{enumerate}
\vspace{1em} % Add space after the question

% --- QUESTION 2 ---
\item \textbf{(20 points) Soft-Margin SVM and Hyperparameter C}

\begin{enumerate}[a)]
    \item (10 pts) Explain the role of slack variables ($\xi_i$) and the hyperparameter $C$ in a soft-margin SVM. What is the optimization objective for the soft-margin SVM (you can describe it in words or as a formula)?
    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}

        
    \item (10 pts) Describe the difference between a classifier trained with a \textbf{very small $C$} versus one trained with a \textbf{very large $C$}. How does this choice affect:
    \begin{itemize}
        \item The width of the margin?
        \item The number of support vectors?
        \item The model's "willingness" to misclassify training points?
        \item The model's position in the bias-variance trade-off?
    \end{itemize}

    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
        

\end{enumerate}
\vspace{1em}

% --- QUESTION 3 ---
\item \textbf{(15 points) The Kernel Trick}

\begin{enumerate}[a)]
    \item (5 pts) What problem in classification do non-linear kernels (the "kernel trick") solve?
    \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
    \item (10 pts) Imagine you have a 1D dataset with points:
    \begin{itemize}
        \item Class +1: \texttt{\{-4, 4\}}
        \item Class -1: \texttt{\{-1, 1\}}
    \end{itemize}
    This data is not linearly separable in 1D. Describe how a simple polynomial kernel, $\phi(x) = x^2$, would transform this data to make it linearly separable in a new dimension. Show the new values for the transformed points and state the simple linear rule (e.g., "classify as +1 if...") that could separate them in the transformed space.
        \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
\end{enumerate}
\vspace{1em}

% --- QUESTION 4 ---
\item \textbf{(15 points) The Dual Problem and Support Vectors}

In the dual formulation of the soft-margin SVM, each data point $\vect{x}_i$ has a corresponding dual variable $\alpha_i$. Based on the Karush-Kuhn-Tucker (KKT) conditions, the value of $\alpha_i$ is constrained by $0 \le \alpha_i \le C$.

For each of the following three cases, state the value or range of the dual variable $\alpha_i$ and explain \textit{what} that value tells us about the point's position relative to the hyperplane and margin:
\begin{itemize}
    \item \textbf{Case 1:} The point $\vect{x}_i$ is correctly classified and is \textbf{outside} the margin.
    \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
    \item \textbf{Case 2:} The point $\vect{x}_i$ is \textbf{on} the margin.
        \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    \item \textbf{Case 3:} The point $\vect{x}_i$ is \textbf{within} the margin or on the wrong side of the hyperplane (i.e., it is a margin violation).
        \begin{tcolorbox}[fit,height=4cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
\end{itemize}

\end{enumerate}

\newpage % Start the next part on a new page

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PART 2: CLUSTERING (K-MEANS AND GMM)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Part 2: Clustering (K-Means and GMM)}

\begin{enumerate}
\setcounter{enumi}{4} % Continue numbering from Part 1

% --- QUESTION 5 ---
\item \textbf{(15 points) K-Means Algorithm Step-by-Step}

You are given $K=2$ and a 2D dataset with four points:
\begin{itemize}
    \item $P_1 = (1, 1)$
    \item $P_2 = (1, 3)$
    \item $P_3 = (4, 2)$
    \item $P_4 = (4, 4)$
\end{itemize}
Your initial centroids are $\vect{c}_1 = (1, 2)$ and $\vect{c}_2 = (4, 3)$.

Perform \textbf{one full iteration} of the K-means algorithm. You must show your work for both steps:
\begin{enumerate}[a)]
    \item (8 pts) \textbf{Assignment Step:} Assign each of the four points ($P_1, P_2, P_3, P_4$) to its nearest centroid ($\vect{c}_1$ or $\vect{c}_2$). Use Euclidean distance.

    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
    \item (7 pts) \textbf{Update Step:} Calculate the new centroids (let's call them $\vect{c}'_1$ and $\vect{c}'_2$) by taking the mean of all points assigned to each cluster in step (a).
    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
\end{enumerate}
\vspace{1em}

% --- QUESTION 6 ---
\item \textbf{(20 points) K-Means vs. Gaussian Mixture Models (GMM)}

\begin{enumerate}[a)]
    \item (10 pts) Compare K-Means and GMM in terms of their cluster assignments. Specifically, explain the difference between a "hard" assignment and a "soft" assignment.

    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
    \item (10 pts) Describe or draw a simple 2D dataset (e.g., with two clusters) where you would expect GMM to perform significantly better than K-Means. Explain \textit{why} K-Means would struggle and what assumption GMM makes that allows it to model this data more effectively. (Hint: think about cluster shapes or density).

    \begin{tcolorbox}[fit,height=7cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution 
    \end{tcolorbox}
    
\end{enumerate}

\end{enumerate}

\end{document}
% --- DOCUMENT END ---
